{"pages":[{"title":"About","text":"Tracy Liu","link":"/about/index.html"}],"posts":[{"title":"Residual Learning for Image Recognition","text":"CVPR 2016@microsoft.com Contentspdf Abstract residual networks VGG nets COCO object detection dataset 1. IntroductionIs learning better networks as easy as stacking more layers? Problem: vanishing/exploding gradients Hamper convergence from the beginning Solutions: normalized initialization and itermediate normalization layers Tens of layers converge with SGD and back propagation Degradation problem With the network depth increasing, accuracy gets saturated and then degrades rapidly Not caused by overfitting Adding more layers to a suitably deep model leads to highter training error Deep residual learning framework desired underlying mapping $H(x)$ another mapping $F(x):=H(x)-x$ original mapping recasting $F(x)+x$ If an identity mapping were optimal, it would be easier to push the residual to zero than to fit an identity mapping by a stack of nonlinear layers identity mapping: $F(x)=x$ Shortcut connections Shortcut connections are those skipping one or more layers, simply perform identity mapping ImageNet Residual nets: easy to optimize, plain nets: higher training error Accuracy 152-layer residual net (deepest) Lower complaxity than VGG nets Error 3.57% 2. Related WorkResidual Representations Powerful shallow representations for image retrieval and classification VLAD: encodes by the residual vectors with respect to a dictionary Fisher Vector: probabilistic version of VLAD Encoding residual vectors is shown to be more effective than encoding original vectors For solving Partial Differential Equations (PDEs) Multigrid method Hierarchical basis preconditioning Shortcut Connections Learning residual functions -&gt; identity shortcuts are never closed 3. Deep Residual LearningResidual Learning Hypothesizes multiple nonlinear layers can asymptotically approximate complicated functions they can asymptotically approximate the residual functions If the added layers can be constructed as identity mappings, a deeper model should have training error no greater than its shallower counterpart Residual learning reformulation If identity mappings are optimal, the solvers may simply drive the weights of the multiple nonlinear layers toward zero to approach identity mappings Identity Mapping by Shortcuts$y = F(x, {W_i}) + x$ $y = F(x, {W_i}) + W_s x$ $W_s$ for matching dimensions Network Architectures Plain Network Based on VGG-net Design rules same output feature map size same number of filters If the feature map size is halved, the number of filters is doubled Residual Network based on plain network add some shortcuts and turn it into residual counterpart version identity mapping doesn’t increase the parameter in the network use project matrix to make the dimension match between $F(x)$ and $x$ Implementation Batch normalization SGD Learning rate: 0.1 Weight decay of 0.0001 and a momentum of 0.9 Don’t use dropout, because the percentage of parameters in the fully connected layer is low. (about 0.01%) 4. Experiments Plain Networks Degradation problem Residual Networks Three major observations 34-layer ResNet is better than the 18-layer ResNet Training error successfully reduced The 18-layer ResNet converges faster (providing faster convergence at the early stage) Identity vs. Projection Shortcuts (A) Zero-padding (B) Projection for increasing dimensions and other are identity (C) All are projections C &gt; B &gt; A (slightly) Deeper Bottleneck Architectures 50-layer ResNets 101-layer and 152-layer ResNets Reference ITREAD Deep Residual Net 分析和实现 論文筆記","link":"/2019/07/17/2019-07-17-Residual-Learning-for-Image-Recognition/"},{"title":"Sequence to Sequence Learning with Neural Networks","text":"NIPS 2014@google.com Contentspdf Abstract Task: an English to French translation task from the WMT-14 dataset Method: a Deep LSTM: maps theinput sequence to a vector of a fixed dimensionality another Deep LSTM: decodes the target sequence from the vector Result: BLEU score 34.8 Additional Founds: reversing the order of the words in all source sentences (but not target sentences) improved the LSTM’s performance markedly 1 Introduction DNNs’ inpus and targets: encoded with vectors of fixed dimensionality. The idea: use one LSTM to read the input sequence, one timestep at a time, to obtain large fixed-dimensional vector representation, and then to use another LSTM to extract the output sequence from that vector 2 The modelRNN A natural generalization of feedforward neural networks to sequences Difficult to train the RNNs due to the resulting long term dependencies LSTM 2 LSTM: one for the input sequence and another for the output sequence Deep LSTMs with 4 layers Reverse the order of the words of the input sentence 3 Experiments3.1 Dataset details 348M French words 304M English words Vector representation 160000 most frequent words for the source language (English) 80000 most frequent words for the target language (French) Out-of-vocabulary: UNK 3.2 Decoding and Rescoring Maximizing the log probability of a correct translation T given the source sentence S Beam search decoder 3.3 Reversing the Source Sentences original source: has a large “minimal time lag” reversed source: the first few words in the source language are very close to the first few words in the target language 3.4 Training details LSTMs 4 layers 1000 cells Parameters: uniform distribution between -0.08 and 0.08 input vocabulary: 160,000 output vocabulary: 80,000 SGD, without momentum, learning rate 0.7 7.5 epochs, after 5 epochs, halving the learning rate every half epoch Batches of 128 Exploding gradients prevending Roughly same-length-sentences in the minibatch 3.5 Parallelization 8-GPU machine 4 for 4 layers of LSTMs 4 for computing the softmax Training took about a ten days 3.6 Experimental Results 3.7 Performance on long sentences 3.8 Model Analysis Turn a sequence of words into a vector Clustered by meaning Sensitive to the order of words Insensitive to the active and passive 4 Related work RNN-Language Model (RNNLM) Feedforward Neural Network Language Model (NNLM) Auli et al: combine an NNLM with a topic model of the input sentence Kalchbrenner and Blunsom: map sentences to vectors using convolutional neural networks -&gt; lose the ordering 5 Concludsion A large deep LSTM with a limited vocabulary does well Improved by reversing the words -&gt; has the greatest number of short term dependencies LSTM can correctly translate very long sentences Reference LSTM Sequence to squence beam search SMT PCA","link":"/2019/07/25/2019-07-25-Sequence-to-Sequence-Learning-with-Neural-Networks/"},{"title":"Auto-Encoding Variational Bayes","text":"ICLR 2014@google.com Contentspdf Introduction Variational Bayesian (VB) Approximate posterior using MLP Stochatic Gradient Variational Bayes (SGVB) Auto-Encoding VB (AEVB) algorithm Variational auto-encoder (VAE) Methodproblem the integral of the marginal likelihood $p_\\theta(x) = \\int p_\\theta(z)p_\\theta(x|z) dz$ is intractable a large dataset: the need of updating using small minibatches graphic Sold lines: generative model $p_\\theta(z)p_\\theta(x|z)$ Dashed lines: variational approximation $q_\\phi(z|x)$ to the posterior $p_\\theta(z|x)$ encoder$q_\\phi(z|x)$: an approximation to the intractable true posterior $p_\\theta(z|x)$ decoder$p_\\theta(x|z)$ The variational boundthe marginal likelihoods of $x^{(i)}$$log\\ p_\\theta(x^{(i)}) = log\\ p_\\theta(x^{(i)},z) - log\\ p_\\theta(z|x^{(i)})$ $log\\ p_\\theta(x^{(i)}) = D_{KL}(q_{\\phi}(z|x^{(i)})||p_{\\theta}(z|x^{(i)})) + \\mathcal{L}(\\theta,\\phi;x^{(i)})$ $log\\ p_\\theta(x^{(i)}) \\geq \\mathcal{L}(\\theta,\\phi;x^{(i)}) = \\mathbb{E}{q_{\\phi}(z|x^{(i)})}[-log\\ q_\\phi(z|x^{(i)}) + log\\ p_{\\theta}(x^{(i)},z)]$ $\\mathcal{L}(\\theta,\\phi;x^{(i)}) = \\mathbb{E}{q_{\\phi}(z|x^{(i)})}[-log\\ q_\\phi(z|x^{(i)}) +$ $log\\ p_{\\theta}(x^{(i)}|z) + log\\ p_{\\theta}(z)$ $]$ $\\mathcal{L}(\\theta,\\phi;x^{(i)}) = -D_{KL}(q_\\phi(z|x^{(i)}) || p_\\theta(z)) + \\mathbb{E}{q_{\\phi}(z|x^{(i)})}[log\\ p_\\theta(x^{(i)}|z)]$ The SGVB estimator and AEVB algorithmdo gradient ascent differentiable transformation for $z$reparameterization $\\tilde{z} = g_\\phi(\\epsilon, x)$ with $\\epsilon \\sim p(\\epsilon)$ Monte Carlo estimates$\\mathbb{E}{q_\\theta(z|x^{(i)})}[f(z)] = \\mathbb{E}_{p(\\epsilon)}[f(g_\\phi(\\epsilon, x^{(i)}))] \\simeq \\frac{1}{L} \\Sigma_{l=1}^L f(g_\\phi(\\epsilon^{(l)}, x^{(i)}))$ where $\\epsilon^{(l)} \\sim p(\\epsilon)$, sampling $L$ datapoints generic SGVBfrom $\\mathcal{L}(\\theta,\\phi;x^{(i)}) = \\mathbb{E}{q_{\\phi}(z|x^{(i)})}[-log\\ q_\\phi(z|x^{(i)}) + log\\ p_{\\theta}(x^{(i)},z)]$ to $\\tilde{\\mathcal{L}}^A(\\theta,\\phi;x^{(i)}) = \\frac{1}{L}\\Sigma_{l=1}^Llog\\ p_\\theta(x^{(i)}, z^{(i,l)}) - log\\ q_\\phi(z^{(i,l)}|x^{(i)})$ where $z^{(i,l)} = g_\\phi(\\epsilon^{(i,l)}, x^{(i)})$ and $\\epsilon^{(l)} \\sim p(\\epsilon)$ second version of the SGVB $D_{KL}(q_\\phi(z|x{(i)})||p_\\theta(z))$ can be integrated analytically has less variance than the generic one from $\\mathcal{L}(\\theta,\\phi;x^{(i)}) = -D_{KL}(q_\\phi(z|x^{(i)}) || p_\\theta(z)) + \\mathbb{E}{q_{\\phi}(z|x^{(i)})}[log\\ p_\\theta(x^{(i)}|z)]$ to $\\tilde{\\mathcal{L}}^B(\\theta,\\phi;x^{(i)}) = -D_{KL}(q_\\phi(z|x^{(i)})||p_\\theta(z)) + \\frac{1}{L}\\Sigma_{l=1}^L(log\\ p_\\theta(x^{(i)}|z^{(i,l)}))$ where $z^{(i,l)} = g_\\phi(\\epsilon^{(i,l)}, x^{(i)})$ and $\\epsilon^{(l)} \\sim p(\\epsilon)$ multiple datapoints$N$ datapoints with $M$ features ($x^{(1)} \\sim x^{(M)}$ ) $\\mathcal{L}(\\theta,\\phi;X) \\simeq \\tilde{\\mathcal{L}}^M(\\theta,\\phi;X^M) = \\frac{N}{M}\\Sigma_{i=1}^M \\tilde{\\mathcal{L}}(\\theta,\\phi;x^{(i)})$ AEVB VAE$log\\ q_\\phi(z|x^{(i)}) = log\\ N(z;\\mu^{(i)},\\sigma^{2(i)}I)$ $\\mu^{(i)}$ and $\\sigma^{(i)}$ are output from MLPMLP: nonlinear function of $x^{(i)}$ and $\\phi$ sampling $z^{(i, l)}$$z^{(i, l)} \\sim q_\\theta(z|x^{(i)})$ $z^{(i, l)} = g_\\phi(x^{(i)}, \\epsilon^{(l)}) = \\mu^{(i)} + \\sigma^{(i)} \\times \\epsilon^{(l)}$ where $\\epsilon^{(l)} \\sim N(0, I)$ $\\mathcal{L}(\\theta,\\phi;x^{(i)}) \\simeq \\frac{1}{2} \\Sigma_{j=1}^J(1 + log((\\sigma_j^{(i)})^2) - (\\mu_j^{(i)})^2 - (\\sigma_j^{(i)})^2) + \\frac{1}{L}\\Sigma_{l=1}^Llog\\ p_\\theta(x^{(i)}|z^{(i,l)})$ Experimentslikelihood lower bound marginal likelihood Reference VAE","link":"/2019/07/31/2019-07-31-Auto-Encoding-Variational-Bayes/"},{"title":"Attention is All You Need","text":"NIPS 2017@google.com Contentspdf Introduction Recurrent Models sequence operations hard to do parallelization the Transformer no recurrence relying entirely on an attention mechanism global dependencies parallelization with 8 GPUs Self-attentionan attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence Model Architecture $d_{model}=512$ Encoder $N$ = 6 Decoder $N$ = 6 masking: self-attention layer is only allowed to attend to earlier positions in the output sequence AttentionScaled Dot-Product Attention $Attention(Q, K, V) = softmax(\\frac{QK^\\top}{\\sqrt{d_k}})V$ divided by $\\sqrt{d_k}$ to prevent extremely large Multi-Head Attention $MultiHead(Q, K, V) = Concat(head_1, …, head_h)W^O$ where $head_i = Attention(QW_i^Q, KW_i^K, VW_i^V)$ in the work $h = 8$ $d_k = d_v = d_{model}/h = 64$ $h$ projections for doing parallelization Applications encoder-decoder attention queries: from previous decoder layer keys and values: from encoder allows every position in the decoder to attend over all positions in the input sequence encoder self-attention decoder self-attention to prevent leftward information flow: masking out (setting to -inf) Positional Encoding$PE_{(pos,2i)} = sin(pos/10000^{2i}/d_{model})$ $PE_{(pos,2i+1)} = cos(pos/10000^{2i}/d_{model})$ $pos$: position $i$: dimension Why Self-Attention Trainingdataset WMT 2014 English-German dataset 4.5 million sentence pairs vocabulary: about 37000 tokens WMT 2014 English-French dataset 36M sentences vocabulary: 32000 tokens hardware and schedule 8 GPUs 100000 steps (each took about 0.4 sec) 12 hours big models step time 1.0 sec 300000 steps (3.5 days) optimizer Adam optimizer Results (A) vary the number of attention heads and the attention key and value dimensions(B) reducing the attention key size $d_k$ hurts model quality(C) and (D) show that bigger models are better and dropout is helpful in avoiding over-fitting(E) replacing sinusoidal positional encoding with learned positional embeddings (early identical) ReferenceThe Illustrated Transformer","link":"/2019/08/16/2019-08-16-Attention-is-All-You-Need/"},{"title":"Effective Approaches to Attention-based Neural Machine Translation","text":"EMNLP 2015@stanford.edu Contentspdf Introduction Attentional mechanism jointly translate and align words 2 types of attention-based models global all source words local subset of source words Neural Machine Translation Probabilistic model$p(y|x)$ input$x_1, …, x_n$ output$y_1, …, y_m$ 2 components encoder: to compute a representation $s$ of source sentences decoder: to generate one target word at a time $log\\ p(y|x) = \\Sigma_{j=1}^m log\\ p(y_j|y_{&lt;j}, s)$ using LSTMthe probability of decoding each word $y_j$$p(y_j|y{&lt;j}, s) = softmax(g(h_j))$ $g$ is the transformation fuction that outputs a vocabulary-sized vector training objective$J_t = \\Sigma_{(x,y)\\in\\mathbb{D}}-log\\ p(y|x)$ Attention-based Models context vector $c_t$ captures relevant source-side information to help predict the current target word $y_t$ weighted average over all the source hidden states align weights $a_t$ $\\tilde{h}_t = tanh(W_c[c_t;h_t])$ $p(y_t|y_{&lt;t},x) = softmax(W_s\\tilde{h}_t)$ Global Attention $a_t(s) = align(h_t, \\bar{h}s) = \\frac{exp(score(h_t, \\bar{h}_s))}{\\Sigma{s’}exp(score(h_t, \\bar{h}_{s’}))}$ $$score(h_t,\\bar{h}_s) =\\begin{cases}h_t^\\top\\bar{h}_s \\h_t^\\top W_a\\bar{h}_s \\v_a^\\top tanh(W_a[h_t^\\top;\\bar{h}_s])\\end{cases}$$ dot, general, concat Local Attention window $D$ $c_t$ is derived as a weighted average within the window $[p_t-D,p_t+D]$ empirically selected $a_t$ fixed dimension $2D+1$ Gaussian distribution centered around $p_t$ $a_t(s) = align(h_t, \\tilde{h}_s)exp(- \\frac{(s - p_t)^2}{2\\sigma^2})$ Monotonic alignment (local-m)$p_t = t$ Predictive alignment (local-p)$p_t = S \\cdot sigmoid(v_p^\\top tanh(W_p h_t))$ source sentence length $S$ model parameters (to be learnt): $v_p^\\top$, $W_p$ Input-feeding Approach to keep tracking of which source words have been translated alignment decisions should be made jointly taking into account past alignment information ExperimentsTraining Details WMT’14 training data 4.5M sentences pairs 116M English words, 110M German words limit vocabularies to 50K with token &lt;unk&gt; LSTM models 4 layers 1000 cells parameters: uniformly initialized in [-0.1, 0.1] 10 epochs using SGD Results","link":"/2019/08/16/2019-08-16-Effective-Approaches-to-Attention-based-Neural-Machine-Translation/"},{"title":"Conditional Image Generation with PixelCNN Decoders","text":"NIPS 2016@google.com Contentspdf Introduction Generate pictures pixel by pixel Related Works PixelRNN: better performance PixelCNN: faster to train (easier to parallelize) Gated PixelCNN Conditional variant of the Gated PixelCNN PixelRNN and PixelCNNThe Distribution of PixelRNNs$p(x) = \\Pi_{i=1}^{n^2}p(x_i|x_1, …,x_{i-1})$ $x$: input picture $x_i$: a single pixel Maskingto make sure the CNN can only use information about pixels above and to the left of the current pixel 3 Color Channels B conditioned on (R, G) G conditioned on R first layer: mask A, otherwise: mask B Gated PixelCNNGated Convolutioal LayersGated Activation Unit $y = tanh(W_{k,f} * x) \\odot \\sigma(W_{k,g} * x)$ $k$: the number of the layer $\\odot$: element-wise product $*$: convolution operator Blind spot A single layer block of a Gated PixelCNN Notations green: convolution operations red: element-wise multiplications and additions blue: splites feature maps Left part: vertical stack Right part: horizontal stack Conditional PixelCNN$p(x|h) = \\Pi_{i=1}^{n^2} p(x_i|x_1, …, x_{i-1},h)$ $h$: a latent vector, image description $y = tanh(W_{k,f} * x + V_{k,f}^T h) \\odot \\sigma(W_{k,g} * x + V_{k,g}^T h)$ Applications of $h$ class dependent bias what should be in the image location dependent bias where Location Dependentmapping $h$ to a spatial representation $s = m(h)$where $s$ has the same width and height as the image $y = tanh(W_{k,f} * x + V_{k,f} * s) \\odot \\sigma(W_{k,g} * x + V_{k,g} * s)$ PixelCNN Auto-Encoders Replacing the deconvolutional decoder with a conditional PixelCNN ExperimentsUnconditional Modeling with Gated PixelCNNPerformance of different models on CIFAR-10 Performance of different models on ImageNet Conditioning on ImageNet Classes Conditioning on Portrait Embeddings PixelCNN Auto-Encoder Reference Gated CNN Pixel RNN (paper) GTU vs. GLU Language Modeling with Gated Convolutional Networks","link":"/2019/08/30/2019-08-30-Conditional-Image-Generation-with-PixelCNN-Decoders/"},{"title":"Google vim-codefmt","text":"Google vim-codefmt Commands:FormatCode &lt;TAB&gt; InstallationWith vim-plugvim ~/.vimrc 1234567call plug#begin('~/.vim/plugged')Plug 'google/vim-maktaba'Plug 'google/vim-codefmt'Plug 'google/vim-glaive'call plug#end() :PlugInstall Autoformattingvim ~/.vimrc 12345678910augroup autoformat_settings autocmd FileType bzl AutoFormatBuffer buildifier autocmd FileType c,cpp,proto,javascript AutoFormatBuffer clang-format autocmd FileType dart AutoFormatBuffer dartfmt autocmd FileType go AutoFormatBuffer gofmt autocmd FileType gn AutoFormatBuffer gn autocmd FileType html,css,sass,scss,less,json AutoFormatBuffer js-beautify autocmd FileType java AutoFormatBuffer google-java-format autocmd FileType python AutoFormatBuffer yapfaugroup END Installing Formattersclang-formatsudo apt install clang-format vim ~/.vimrc 12call glaive#Install()Glaive codefmt clang_format_executable='/usr/bin/clang-format'","link":"/2019/09/06/2019-09-06-Google-vim-codefmt/"},{"title":"Learning Deep Features for Discriminative Localization","text":"CVPR 2016@csail.mit.edu Contentspdf IntroductionProblemIn some CNN models, the objects localization ability is lost when fully-connected layers are used for classification GoalTo both classify the image and localize class-specific image regions in a single forward-pass Related Works Weakly-supervised object localization Bergamo et al, Cinbis et al, Pinheiro et al, Oquab et al They are not trained end-to-end Visualizing CNNs visualize the internal representation learned by CNNs in an attempt to better understand their properties Zeiler et al, Mahendran et al, Dosovitskiy et al Class Activation Mapping (CAM)using global average pooling (GAP) $f_k(x, y)$: the activation of unit $k$ in the last convolutional layer at spatial location $(x, y)$ $F^k = \\sum_{x, y}f_k(x, y)$: the result of perfoming global average pooling for unit $k$ $w_k^c$: the importance of $F_k$ for class $c$ $S_c = \\sum_kw_k^cF_k$: the input of the softmax $P_c = \\frac{exp(S_c)}{\\sum_c exp(S_c)}$: the output of the softmax for class $c$ $M_c$: class activation map for class c $F^k = \\sum_{x, y}f_k(x, y)$ $S_c = \\sum_k w_k^c \\sum_{x,y} f_k(x, y) = \\sum_{x,y}\\sum_k w_k^cf_k(x,y)$ $M_c(x,y) = \\sum_kw_k^cf_k(x,y)$ $S_c = \\sum_{x,y} M_c(x,y)$ Global Average Pooling vs. Global Max Pooling GAP loss encourages the network to identify the extent of the object GMP loss encourages to identify just one discrimminative part Weakly-supervised Object LocalizationSetup Originals AlexNet VGGnet GoogLeNet Remove the fully-connected layers before the final output and replace them with GAP followed by a fully-conneted softmax layer AlexNet-GAP VGGnet-GAP GoogLeNet-GAP AlexNet*-GAP AlexNet is the most affected by the removal of the fully-connected layers add two convolutional layers just before GAP ResultsClassification In most cases there is a small performance drop of 1 − 2% when removing the additional layers from the various networks Localization Generate bounding boxes: First segment the regions of which the value is above 20% of the max value of the CAM. Then take the bounding box that covers the largest connected component in the seg-mentation map Deep Features for Generic LocalizationFine-grained Recognition Pattern DiscoveryDiscovering informative objects in the scenes Concept localization in weakly labeled images Weakly supervised text detector Interpreting visual question answering Visualizing Class-Specific Units Reference 浅谈Class Activation Mapping（CAM） CAM(Class Activation Mapping) ResNet, AlexNet, VGG, Inception: 理解各种各样的CNN架构 《Self-produced Guidance for Weakly-supervised Object Localization》论文笔记 Note from marmot Note from darklenx","link":"/2019/09/12/2019-09-12-Learning-Deep-Features-for-Discriminative-Localization/"},{"title":"使用 Google Script 來管理 Gmail","text":"Google Apps Script 是 Google 自己出的 Script，可以拿來串接 Google 各項服務的 API 上學期期末的時候其實就聽過系上社團學長說他有用這個在管理他的 Gmail，當然那個時候也是第一次聽過有 Google Script 這種東西 不過期末實在太忙了就沒有研究，然後放暑假 email 們也少少的所以也沒有想到 直到最近開學，直接被各種學校公告學習系統 email 轟炸，熊熊覺得可以來研究一下了 亂摸了幾天總算給我摸出了個小功能ˊˇˋ 就是每天固定時間將一些公告帳號寄來的郵件清掉 建立一個 Script首先要先到 Google Drive 新增一個 Script 檔找不到的話點 連結更多應用程式 裡面會有 打開之後沒意外會是一個專案，裏面有一個.gs檔 Gmail API以下是我.gs檔裡面的內容 123456789101112function deleteThreads() { var searchRules = 'in:inbox -in:starred has:nouserlabels from:' var toDelete = ['sport@cc.nctu.edu.tw', // 交大體育室 'nctuannounce@nctu.edu.tw', // 交大公告 'cdcp_notice@nctu.edu.tw'] // E3公告 for (var i = 0; i &lt; toDelete.length; i ++) { var threads = GmailApp.search(searchRules + toDelete[i]); for (var j = 0; j &lt; threads.length; j ++) { threads[j].moveToTrash(); } }} 簡單翻譯一下，toDelete array 中是我想刪除的郵件們的 sender透過 searchRules 和 toDelete[i] 可以組成一個搜尋條件這個搜尋條件和我們平常打在 Gmail 搜尋框跑出來的搜尋結果一樣詳細資訊可以看看 可搭配 Gmail 使用的搜尋運算子 至於 searchRules 的意思： in:inbox: 在收件夾中，封存的不算 -in:starred: 未加星號（- 代表 not） has:nouserlabels: 沒有自訂 label 接著將搜尋條件塞入 GmailApp.search()結果存進 threads，也就是想刪除的 email 對話串最後透過 threads[j].moveToTrash() 將對話串一個一個刪掉 大概就是這樣想寫其他功能還要再多研究 Gmail API Time Trigger有了刪郵件的功能再來就是新增一個 Time Trigger 來幫我每天刪郵件了 按編輯 &gt; 現有專案的啟動程序 就會跳到 Developer Hub 的頁面接著按 新增觸發條件之後就是愉快的圖形介面設定了大概就是選擇某個 function 還有它的發動時間及頻率儲存之後就大功告成了！","link":"/2019/09/13/2019-09-13-Google-Script-on-Gmail/"},{"title":"Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization","text":"ICCV 2017@vt.edu @gatech.edu Contents slide pdf Introduction Input: Images Output: Visualization of discrimination regions Interpretability Without architectural changes or re-training To give explaination to the prediction of some models General version of CAM Grad-CAMNeuron importance weights $\\alpha_k^c$ class: $c$ the gradient of the score for $c$: $y^c$ feature maps: $A^k$ the number of pixels: $Z$ $$\\alpha_k^c = \\frac{1}{Z} \\sum_i\\sum_j \\frac{\\partial y^c}{\\partial A_{ij}^k}$$ $$L_{Grad-CAM}^c = ReLU(\\sum_k \\alpha_k^cA^k)$$ Compare with CAM the prediction: $S^c$ $$S^c = \\sum_k w_k^c \\frac{1}{Z}\\sum_i\\sum_j A_{ij}^k$$ $$S^c = \\frac{1}{Z}\\sum_i\\sum_j\\sum_k w_k^cA_{ij}^k$$ $$L_{CAM}^c = \\sum_k w_k^cA_{ij}^k$$ GAP $A^k$: a feature map of a convolutional layer $A^k_{ij}$: a pixel value of $A^k$ $F^k$: GAP of the feature map in the last layer $Z = i \\times j$: the number of pixels $$F^k=\\frac{1}{Z}\\sum_i\\sum_jA^k_{ij}$$ Final Result $Y^c$: the prediction $W^c_k$: weights $$Y^c=\\sum_k w^c_k \\cdot F^k$$ From GAP $$\\frac{\\partial Y^c}{\\partial F^k}=\\frac{\\partial Y^c}{\\partial A^k_{ij}}\\frac{\\partial A^k_{ij}}{\\partial F^k}=\\frac{\\frac{\\partial Y^c}{\\partial A^k_{ij}}}{\\frac{\\partial F^k}{\\partial A^k_{ij}}}$$ $$\\Rightarrow \\frac{\\partial Y^c}{\\partial F^k}=\\frac{\\partial Y^c}{\\partial A^k_{ij}}\\cdot Z$$ From Final Result $$\\frac{\\partial Y^c}{\\partial F^k}=\\frac{\\partial \\sum_k w^c_k \\cdot F^k}{\\partial F^k}=w^c_k$$ $$\\Rightarrow w^c_k=\\frac{\\partial Y^c}{\\partial A^k_{ij}}\\cdot Z$$ $$\\sum_i\\sum_jw^c_k=\\sum_i\\sum_j\\frac{\\partial Y^c}{\\partial A^k_{ij}}\\cdot Z$$ $$Z,w^c_k=Z \\sum_i\\sum_j\\frac{\\partial Y^c}{\\partial A^k_{ij}}$$ $$w^c_k=\\sum_i\\sum_j\\frac{\\partial Y^c}{\\partial A^k_{ij}}$$ Reference 使用 Grad-CAM 解釋卷積神經網路的分類依據 Grad-CAM 介紹 — Grad-CAM:Visual Explanations from Deep Networks via Gradient-based Localization 论文翻译】Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization ICCV 2017 黑盒机器学习的可解释性","link":"/2019/09/19/2019-09-19-Grad-CAM-Visual-Explanations-from-Deep-Networks-via-Gradient-based-Localization/"},{"title":"使用 Docker 架設 DOMjudge","text":"基於某些原因社團需要架設 DOMjudge，因為不想弄髒環境所以最後選擇建在 Docker 裏面架設 DOMjudge 需要 3 個以上的 Container，Domjudge 官網上有幫我們整理好 Docker 指令要怎麼下 MariaDB Container1docker run -it --name dj-mariadb -e MYSQL_ROOT_PASSWORD=rootpw -e MYSQL_USER=domjudge -e MYSQL_PASSWORD=djpw -e MYSQL_DATABASE=domjudge -e CONTAINER_TIMEZONE=Asia/Taipei -p 13306:3306 mariadb --max-connections=1000 用來放 DB 的 container 幾乎和官網上的一模一樣，只要多加 -e CONTAINER_TIMEZONE=Asia/Taipei 這項就好 DOMserver Container1docker run -v /sys/fs/cgroup:/sys/fs/cgroup:ro --link dj-mariadb:mariadb -it -e MYSQL_HOST=mariadb -e MYSQL_USER=domjudge -e MYSQL_DATABASE=domjudge -e MYSQL_PASSWORD=djpw -e MYSQL_ROOT_PASSWORD=rootpw -e CONTAINER_TIMEZONE=Asia/Taipei -p 12345:80 --name domserver domjudge/domserver:latest 接著是 DOMserver 本體，也是只要加上 -e CONTAINER_TIMEZONE=Asia/Taipei 就好 Find the Passwords接著跑下面這行進入 DOMserver 虛擬機本體 1docker container exec -it domserver bash 找到 opt/domjudge/domserver/etc/initial_admin_password.secret 和 /opt/domjudge/domserver/etc/restapi.secret 兩個檔案裡面分別是 admin 和 judgehost 的密碼（DOMjudge 7.0.0 之後 admin 密碼就不是預設 admin 了） Judgehost Container1docker run -it --privileged -v /sys/fs/cgroup:/sys/fs/cgroup:ro --name judgehost-0 --link domserver:domserver --hostname judgedaemon-0 -e DAEMON_ID=0 -e CONTAINER_TIMEZONE=Asia/Taipei -e JUDGEDAEMON_PASSWORD=${judgedaemon_password} domjudge/judgehost:latest 其中 ${judgedaemon_password} 要換成剛剛在 DOMserver 本體內撈到的 judgehost 密碼Judgehost 可以架好多個，把上面 name、hostname、DAEMON_ID 的參數調整一下就好 Connect to the Server在瀏覽器中打開 http://localhost:12345 就能連到 DOMjudge的網頁啦要登入 admin 的話要在 user 打 admin，password 就是剛剛撈到的","link":"/2019/09/21/2019-09-21-Use-Docker-to-setup-Domjudge/"},{"title":"Tcsh Prompt 個人化","text":"最近學校好多作業都要使用系上的工作站系上有提供能夠 ssh 進去的 Linux 跟 FreeBSD 空間預設的 shell 是 tcsh而且是很原生的 tcsh 多麼的原生呢？ 這麼的原生QQ，prompt 真的好醜QAQ最致命的是它還是白色的，如果遇到 output 比較多的時候往上找 output 頭還會找到跟丟QAQ 覺得好看的 prompt 真的有必要，所以就去找了要怎麼改 Default Prompt跟 bash 不一樣的是，bash 的 prompt 設定是塞在一個叫 PS1 的環境變數裡，而 tcsh 的環境變數就叫作 prompt，害我一開始搞錯方向ˊˋ echo $prompt 如果是預設的話就會顯示下面這個 1[%n@%m %c]%# 名詞解釋時間： %n: User name %m: The hostname up to the first ‘.’ %c: The trailing component of the current working directory %#: The first character of the promptchars shell variable for normal users. ‘#‘ for the superuser. 更多的可以到這裡看 My New Prompt為了永久設定，要在 home 目錄下建立一個 rc 檔案 vim .cshrc 裡面放入以下的內容： 1234set _green=&quot;%{\\033[38;5;120m%}&quot;set _cyan=&quot;%{\\033[38;5;123m%}&quot;set _white=&quot;%{\\033[0m%}&quot;set prompt=&quot;${_green}%n@%m${_white}:${_cyan}%~${_white}%# &quot; 前兩行是設定顏色的意思\\033[38;5;${color_no}m 是設定 front color 的意思，把 ${color_no} 換成自己想要的顏色就好${color_no} 要去 xterm-color 翻 如果電腦有 ruby 的話，可以下這個指令看 table (reference) 1/usr/bin/ruby -e &quot;$(curl -fsSL https://raw.githubusercontent.com/gawin/bash-colors-256/master/colors)&quot; 跑出來就會像以下這樣 第三行的 %{\\033[0m%} 是將顏色重設回預設的意思 而第四行就是設定 prompt 本體了我把原來的 %c 換成 %~，會顯示比較完整的路徑，我覺得這樣比較能幫助我理解檔案們的相對位置關係 至於格式的部份我設成跟 Ubuntu 一樣，因為看習慣了ˊˇˋ不過顏色我沒有完全照抄，Ubuntu 還有 Bold 的樣式而我只設定了 front color覺得可以跟桌機有點不一樣，可以比較直覺的知道現在在哪裡做事 最後放上我的新 prompt 的截圖 感覺有空的話可以再調一下資料夾跟執行檔的顏色查到的參考連結裡面就有提到作法，還有更詳細的 prompt 個人化的說明","link":"/2019/10/07/2019-10-07-tcsh-prompt-styling/"},{"title":"Datasets","text":"MNIST SeriesMNIST Image of handwritten digit 60000 training examples 10000 test examples 28*28 grayscale images 10 classes Fashion-MNIST Images of clothing and accessories 60000 training examples (same as mnist) 10000 test examples (same as mnist) 28*28 grayscale images. (still the same as mnist) 10 classes. (same as mnist) Designed to be a direct drop-in replacement for MNIST Kuzushiji MNIST (KMNIST) Images of Kanji characters. Also a drop-in replacement of MNIST. Provide Kuzushiji-49 with 49 classes. Provide Kuzushiji-Kanji for all Kanji characters, but less complete. EMNIST Images of handwritten digits &amp; characters Same format as MNIST, but with more classes Designed to be more challenging than MNIST QMNIST Recover missing testing data in MNIST ImagesCOCO Datasets Large scale image dataset object detection, segmentation, and captioning Includes segmentations and bounding boxes of objects in images Includes image captions LSUN Images of scenes or objects 10 scene categories 20 object categories Partially automated labeled. Huge amount (1 million) images per category ImageNet Images organized following WordNet hierarchy (only nouns) Largest image datasets 15 millions of images 22 thousands of classes CIFAR Tiny images of 32*32 coloured images CIFAR-10 : 10 classes with 6000 images each CIFAR-100: 100 classes with 600 images each STL-10 Images of 10 classes Inspired by CIFAR-10 but more - focused on unsupervised tasks Higher resolution: 96*96 coloured images Images acquired from ImageNet Assistance to building priors SVHN Images of street view house numbers Two formats Original images with character level bounding boxes MNIST-like 32*32 image center around a single character SBU Captioned Photo 1 million images with associated visually relevant captions Resources Data 1 million image urls and their captions A script to crawl images Search tool: search for images using text queries The Flickr30k Dataset Pascal VOC Data Sets 20 classes 11530 images 27450 ROI annotated objects 6929 segmentations Challenges Cityscapes Dataset Urban street scenes Features Annotations: semantic, instance-wise, dance pixel 30 Classes 50 cities Different seasons, daytime, and weather conditions 5000 fine annotated images 20000 coarse annotated images Semantic Boundaries Dataset Mark up the pixels that lie on the boundary of the object VideosKinetics-400 Dataset 650000 video clip url links 700 human cation classes Each action class has at least 600 video clips HMDB Dataset 1,000,000,000 videos of human motions General facial actions: smile, laugh, chew, talk Facial actions with object manipulation: smoke, eat, drink General body movements Body movements with object interaction Body movements for human interaction UCF101 Dataset An action recognition data set of realistic action videos 5 action categories Human-Object interaction Body-Motion only Human human interaction Playing musical instruments Sports Reference torchvision.datasets","link":"/2019/11/21/2019-11-21-Datasets/"},{"title":"使用 docker-compose 建立開發環境","text":"動機最近覺得自己的開發技能還有很多欠缺的地方尤其是完整的開發流程以往都是直接在自己的本機開發也沒有使用 CI/CD 然後又突然想寫寫看 OJ於是我打算直接做中學並做以下幾個改變 使用 docker-compose 建立開發環境 善用 environment variables 做 testing 以及 CI/CD 認真寫 git commit messages 將學到的東西跟踩到的坑寫成 blog 文章 大概就是一些提升開發 sense 的東西XD當然這些技能很多都是要從頭學的例如今天的 docker 就是以前使用過別人的但沒有自己寫過 架構暑假的時候大概看過資料一般 OJ 除了傳統上的的前後端還多了一個 judgehost雖然最陌生，但想了想好像 judgehost 比較不複雜所以我決定先從 judgehost 開始寫又因為 judgehost 通常是計算最繁重的所以我選了相對輕量的 Flask 作 serverioi 開源的 isolate 套件作我的 sandbox 這篇主要是記錄我的 docker 環境如何架設 File Structure123456.├── app│ └── our application codes here├── docker-compose.yml├── Dockerfile└── requirements.txt 在 judgehost 目錄下有一個子資料夾 app 和建環境要用的檔案app 下就放了跟 judgehost application 相關的 code（跟 Flask 有關的 code 都在這裡） DockerfileDockerfile 是用來 build docker image 的 123456789101112131415161718192021FROM ubuntu:20.04WORKDIR /appADD requirements.txt /appRUN apt-get update# pip packagesRUN apt-get install -y python3-pipRUN pip3 install -r requirements.txt# utilsRUN apt-get install -y gitRUN apt-get install -y makeRUN apt-get install -y gccRUN apt-get install -y --no-install-recommends vimRUN apt-get install -y --no-install-recommends redis-server# isolateRUN git clone https://github.com/ioi/isolate.gitRUN apt-get install -y libcap-devRUN cd isolate &amp;&amp; make install &amp;&amp; cd ..EXPOSE 5000 FROM: 初始 image，這邊使用跟我桌機一樣的 ubuntu:20.04 當開發環境的 os WORKDIR: 建立一個資料夾並將這個資料夾作為主要工作區（docker 會自動 cd 進去） ADD: 這裡是將 host 中的 requirements.txt 檔案加到 docker image 的 /app RUN: 一些 build image 時才會跑的初始 command，通常都是一些裝東西的用途 EXPOSE: 告訴使用這個 image 的人我們的 application 會使用哪些 port docker-compose.ymldocker-compose 這個 command 是用來將 images 建立成可以跑的 docker containers也就是代替了 docker run 這個指令 1234567891011121314151617181920212223version: '3'services: judgehost: build: context: . environment: - FLASK_APP=judgehost.py - FLASK_RUN_HOST=0.0.0.0 - FLASK_ENV=development - REDIS_DB_HOST=redis - SANDBOX_ROOT=/var/local/lib/isolate/ command: bash -c 'isolate --cg --init &amp;&amp; flask run' ports: - '5000:5000' volumes: - './app:/app' tty: true privileged: true depends_on: - 'redis' redis: image: 'redis:alpine' 這個 docker-compose.yml 檔案定義了 judgehost、redis: 兩個服務，這兩個服務分別會用一個 container 去 run judgehost build: judgehost 這個服務的 image 要用剛剛提到的 Dockerfile 去 build，而 context 的 . 代表 Dockerfile 所在的目錄位置，也就是當前目錄 environment: 一些環境變數 command: 每次跑 docker-compose up 時會在 container 中跑的指令 ports: Port forwarding，host_port:container_port，其中 host 是本機 volumes: 將本機的 ./app 資料夾 mount 進 container 的 /app 資料夾，也就是說這兩邊會同步，改了其中一邊另外一邊也會跟著變動 tty: 這邊設為 true，它的意義我還不是很清楚，但知道它可以讓 container 在執行完 command 後不立即結束 previleged: 讓 container 中的 root 擁有真正的 root 權利 depends_on: 這個服務會需要用到其他哪些服務，這裡因為 judgehost 會需要使用 redis 的服務，docker-compose 在建立的時候就會先建立 redis redis image: 因為是直接使用別人已經建立好的 image，所以就直接設定 image 是什麼而不用 build 了 docker-compose 相關指令1docker-compose build build 會將 docker-compose.yml 中需要用 Dockerfile build 的 images 建立起來，建立好後用 docker images 的指令可以看到 1docker-compose up images 建立好後下這行，會將 container 們建立起來並用虛擬網路連接 1docker-compose up -d 這個功能同上，但是加了 -d 的選項就會在背景執行，也不會有 log 跑出來 1docker-compose down 移除 container 們以及虛擬網路 一些踩到的坑ADD requirements.txt我一開始以為只要透過 volumes 那邊將本機的檔案 mount 過去再在 container 那邊 RUN pip3 install -r requirements.txt 就可以把 image build 起來但實際上在 build 時只跟 Dockerfile 有關而與 docker-compose.yml 無關所以在 build 階段是看不到即將要被 mount 上去的檔案的因此需要 ADD requirements.txt 將 requirements.txt 加到 container 的 WORKDIR 中再去執行 pip3 FLASK_RUN_HOST=0.0.0.0docker 在做 port forwarding 的時候預設是將 0.0.0.0:container_port 拉到 host_port但 Flask 預設是跑在 127.0.0.1所以要多設定 FLASK_RUN_HOST 的環境變數讓它可以跑在 0.0.0.0 才 forward 得出來 碎碎唸其實 docker 這個東西之前在參加社團時已經用過了但是完全都是社團學長們寫好我們只要把用他們的東西把環境架起來而已真的不知道裡面在幹嘛XD自己寫一遍真的有恍然大悟的感覺 下一篇應該會來寫寫 sandbox 怎麼用希望這個禮拜有時間寫 2021.1.3 Updated這個學期修了一門架雲服務的課，認識了人很好的助教，助教學姐跟我說 Dockerfile apt-get 的部份寫在同一行並刪掉 apt lists 可以讓 docker 的 cache 大小減小 下面是那堂課架的服務的 Dockerfile 來當作例子 主要的變動有 apt-get 全寫在同一行（用 \\ 連接起來） apt clean rm -rf /var/lib/apt/lists/* 123456789101112131415FROM ubuntu:20.04WORKDIR /codeADD requirements.txt /codeRUN apt-get update &amp;&amp; \\ apt-get install -y --no-install-recommends python3-pip \\ vim \\ python3-pymysql &amp;&amp; \\ apt clean &amp;&amp; \\ rm -rf /var/lib/apt/lists/*RUN pip3 install -r requirements.txtEXPOSE 5000","link":"/2020/09/21/2020-09-21-Use-docker-compose-to-Build-Development-Environment/"},{"title":"isolate Sandbox 使用","text":"SandboxSandbox 的中文名字就是直譯過來的沙箱他能讓程式在一個隔離的環境中執行隔離的意思就是有限的路徑、有限的記憶體甚至是有限的 process 數量等等通常是拿來測試一些不受信任的程式讓我們就算執行了惡意程式也不會影響到我們的作業系統或環境 今天依然是在寫 OJ 中 judgehost 的部份因為 judgehost 需要跑別人 submission 送來的程式如果有人送個惡意程式的話就糟糕了所以就需要用到 sandbox把大家的 submission 放在 sandbox 裡面跑 isolateisolate 是 ioi 的那群人開發的一個開源 sandbox甚至還有蠻不錯的 manual page前陣子發現還不錯的 judgehost 開源 API —— Judge0 也是使用 isolate 搭配 Ruby on Rails 實現的 Installation首先是安裝 123$ git clone https://github.com/ioi/isolate.git$ apt-get install -y libcap-dev$ cd isolate &amp;&amp; make install 其中 libcap-dev 是它需要的 dependency另外他還需要 make 跟 gcc 能讓 make install 這個指令順利跑沒有的話要裝一下 接下來是一點關於 docker 的部份 Installation 這部份其實在前一篇的 docker 設定中有稍微出現過所以可以在那篇 Dockerfile 的 configuration 中看到幾乎一模一樣的： 1234# isolateRUN git clone https://github.com/ioi/isolate.gitRUN apt-get install -y libcap-devRUN cd isolate &amp;&amp; make install &amp;&amp; cd .. 如果要在 docker container 中跑的話docker run 這個指令要記得加 --privileged 的參數或者在 docker-compose.yml 中把 privileged 設為 true讓 docker root 擁有真正 root 的權限才能創建 cgroup Initialization1$ isolate --cg --init 這個指令下下去後會 stdout 給我們一個路徑，通常是 /var/local/lib/isolate/0而 /var/local/lib/isolate/0/box （這裡多了一個 box 喔！）就會是 sandbox 能夠運用的路徑白話一點的意思就是所有在 sandbox 裡的程式都只能跑在這個資料夾下面 這邊特別要提一下在 manual page 的最前面 initialization 的指令只有 isolate --init 而已，這個指令一樣會回給我們路徑但是 isolate --init 的 sandbox 只能跑單一 process 的程式而因為我 sandbox 內會需要跑類似 g++ test.cpp 這樣子之類的指令這些指令需要不只一個 process （會有 fork）所以我們需要 cgroup 幫我們把資源切好出來給我們跑才行這裡的 --cg 就是要使用到 cgroup 的意思 Runisolate 能直接下 --run 跑的檔案只能是執行檔下面來舉一個例子 compile12#!/bin/bashg++ test.cpp compile 是一個可執行的 shell script （chmod +x compile）裡面的動作是用 g++ 編譯 test.cpp 這個檔案 test.cpp123456#include &lt;bits/stdc++.h&gt;using namespace std;int main() { cout &lt;&lt; &quot;Hello C++&quot; &lt;&lt; endl;} 以上是 test.cpp 的檔案內容 12$ isolate --run -p -E PATH=$PATH --meta=meta.txt -- compileOK (0.831 sec real, 0.851 sec wall) 直接執行這個指令，成功執行的話就會得到 OK 並在 /var/local/lib/isolate/0/box 的路徑下出現編譯好的 a.out 參數 -p: 允許多個 process -E: 指定 environment variable，我這邊把 PATH 直接送進去讓它可以直接使用 g++ --meta: sandbox 程式執行的情況，裡面會有 exitcode 之類的資訊，如果有用 manual page 中一些時間或空間上的限制，還會出現以下的 status 資訊 RE: run-time error, i.e., exited with a non-zero exit code SG: program died on a signal TO: timed out XX: internal error of the sandbox 以下是上面那個例子的 meta.txt，注意 meta.txt 會出現在下指令的當前目錄的相對位置而不是 sandbox 中 meta.txt123456time:0.824time-wall:0.841max-rss:154216csw-voluntary:19csw-forced:17exitcode:0 Clean Up1isolate --cg --cleanup 下這個就可以把 sandbox 移除了注意這個指令會讓 /var/local/lib/isolate/0 整個不見","link":"/2020/09/22/2020-09-22-Sandbox-ioi-isolate/"},{"title":"使用 travis-ci 自動部署 Hexo Blog 到 GitHub Page","text":"Hexo 部署到 GitHub PageHexo 部署到 GitHub Page 應該算是最常見的方式了我之前都是照這篇最後面的方式部署但我在去年決定改成使用 travis-ci 來幫我自動化的將部落格丟到伺服器上 我的 Hexo Blog git 架構我延續了使用叫做 &lt;username&gt;.github.io 的 repository，Branch 名稱與規劃如下： Branch site: source，markdown 稿子 Branch master: 由 hexo generate 產生的靜態檔案（伺服器位置 所以我的 travis-ci 需要做的事有 在 site 這個 branch 將 npm 的 dependencies 安裝好 在 site 這個 branch 使用 hexo generate 來產生靜態檔案 將 hexo generate 產生的資料夾 public 整包內容丟到 branch master travis-ci 設定取得 GitHub Personal Access TokenSettings &gt; Developer settings &gt; Personal access tokens 點選 Generate new token，勾選 public_repo 之後產生 token 將 Token 當作環境變數放入 travis-ci 環境找到相對應的 repository 點選 settings 接著找到 Environment Variables 的區塊，將剛剛產生的 token 內容複製進去並命名為 GITHUB_TOKEN .travis.yml 內容接下來我們要在 branch site 這邊的根目錄下新增一個叫 .travis.yml 的檔案，之後 travis-ci 在我們每次 push 之後都會自己去看這個檔案，執行我們 deploy 想做的動作 123456789101112131415161718192021222324sudo: falselanguage: node_jsnode_js: - 12cache: - npmbranches: only: - sitescript: - npm install - hexo generatedeploy: provider: pages skip-cleanup: true github-token: $GITHUB_TOKEN keep-history: false committer_from_gh: true target_branch: master on: branch: site local-dir: public Results完成之後我們在每次 push 之後來 https://travis-ci.com/github/\\&lt;username&gt;/&lt;username&gt;.github.io這個網址看我們的 deploy 有沒有成功還有 deploy 的進度如果出現下圖綠色的樣子就代表成功了！","link":"/2021/03/22/2021-03-24-Use-travis-ci-to-Deploy-Blog/"},{"title":"實作 Flask 後端中的 Hash ID 需求","text":"Hash ID上個學期有個小組的學期作業，要寫一個雲端服務，我們組的主題是實作出一個功能較多又比較兼顧 UI 的 when2meet 在實作分享功能的時候，我們希望有一個頁面的資訊（一個叫做 meeting 的 model）可以用網址分享，讓沒登入的人也可以看得到，但是這樣的分享方式如果直接用 restful api 的標準的話，直接使用 api/meetings/&lt;meeting_id&gt; （這裡 &lt;meeting_id&gt; 是整數）當網址來呼叫需要的 api，會讓不相干的人直接在 &lt;meeting_id&gt; 打上數字戳進來 所以要使用 hash_id 來實作，最簡單的方式是直接隨機 random 出一串字串當作 model 的 id，但我總覺得這樣做很不舒服，所以我選擇直接拿原來的 id 做 encrypt 流程會變成像下面這樣： 後端 -&gt; id -(encrypt)-&gt; hash_id -&gt; user 至於 encrypt 的方式我選擇使用最簡單的 DES，多虧大二時的密碼學有稍微認真一下才知道這個名詞XD DES Encrypt/Decrypt using Python這個 section 要來記錄一下如何用 python 從 integer id 轉換成 hash_id 的字串（16 進位表示），又要如何從 hash_id 轉回 id Installation1pip3 install pycryptodome Import12import binasciifrom Crypto.Cipher import DES Encrypt (id -&gt; hash_id)1234567891011121314151617181920id = 1secret_key = '12345678' # 設定一個 secret_key，長度只能為 8secret_key = secret_key.encode() # secret_key = b'12345678'，將 secret_key 從 string encrypt 成 bytesdes = DES.new(secret_key, DES.MODE_ECB) # 建立一個 DES 物件plain = str(id) # 將 id 轉成 stringplain = (16 - len(plain)) * '0' + plain # 前面補零補到 16 個位數enc = des.encrypt(plain.encode()) # 一樣將 plain 轉乘 bytes 之後再做 DES encrypt（des 只吃 bytes）print(enc) # b'_j}R\\x8e9O9&lt;\\x11H\\xc3U\\x0b8\\x1f'enc = binascii.hexlify(enc) # 因為 des 回傳的東西是 bytes 編碼非常不好看所以我們再使用 hexlify 的編碼讓它看起來只有數字 0~9 還有字母 a~fprint(enc) # b'5f6a7d528e394f393c1148c3550b381f'enc = enc.decode() # 需要用 bytes 編碼的部分結束了，將它轉回 stringprint(enc) # 5f6a7d528e394f393c1148c3550b381fenc = enc.upper() # 個人喜好喜歡把他們都轉成大寫，不轉也可以print(enc) # 5F6A7D528E394F393C1148C3550B381F 注意 des.encrypt() 跟 binascii.hexlify() 傳入的東西都需要是 bytes 編碼 Decrypt (hash_id -&gt; id)知道 encrypt 的步驟後 decrypt 就相對簡單了，倒著做回來就好 1234567891011hash_id = '5F6A7D528E394F393C1148C3550B381F'secret_key = '12345678' # 設定一個 secret_key，要跟 encrypt 時相同secret_key = secret_key.encode() # secret_key = b'12345678'，將 secret_key 從 string encrypt 成 bytesdes = DES.new(secret_key, DES.MODE_ECB) # 建立一個 DES 物件dec = binascii.unhexlify(hash_id.lower().encode())dec = des.decrypt(dec).decode()id = int(dec) # id = 1 Flask Model再來以當時那個 project 當例子，來看看在 Flask 中是怎麼實作的，完整版請點這裡 123456789101112131415161718192021222324class Meeting(db.Model): id = db.Column(db.Integer, primary_key=True) # 其他 attributes 在這裡先略過 @property def hash_id(self): des = DES.new(app.config['MEETING_HASH_KEY'], DES.MODE_ECB) plain = str(self.id) plain = (16 - len(plain)) * '0' + plain enc = des.encrypt(plain.encode()) enc = binascii.hexlify(enc).decode().upper() return enc @staticmethod def get_id(hash_id): des = DES.new(app.config['MEETING_HASH_KEY'], DES.MODE_ECB) try: dec = binascii.unhexlify(hash_id.lower().encode()) dec = des.decrypt(dec).decode() id = int(dec) except: return None return id 基本上跟前一個 section 一樣，只是 get_id decrypt 的時候用 try 多檢查一下有沒有 decrypt 成功 在 hash_id 那個 function 我加上了 decorator @property，將 hash_id 設為只能讀取不能修改，因為雖然它的意義很像一個真的存在的 attribute，因為實際上並沒有一個叫 hash_id 的 attribute，它的值完全是由 id 轉過來的 另外 @property 還有一個好處，在使用 Meeting 這個 class 時要像下面這樣 12meeting = Meeting()print(meeting.hash_id) 注意 hash_id 後面沒有括號，意義上更像一個 attribute 而不是一個動作 (function) 了，真棒/ 更多有關 @property 的說明可以在這裡找到 而 get_id 我加上了 @staticmethod 的 decorator，用法就可以像下面這樣 1id = Meeting.get_id(hash_id) 差別在於加上 @staticmethod 就不用每次 get_id 都創一個物件出來，純呼叫 get_id 的 function 就好","link":"/2021/03/30/2021-03-30-The-Implementation-of-Hash-ID-using-Flask/"}],"tags":[{"name":"deep learning","slug":"deep-learning","link":"/tags/deep-learning/"},{"name":"vim","slug":"vim","link":"/tags/vim/"},{"name":"gs","slug":"gs","link":"/tags/gs/"},{"name":"organize","slug":"organize","link":"/tags/organize/"},{"name":"docker","slug":"docker","link":"/tags/docker/"},{"name":"DOMjudge","slug":"DOMjudge","link":"/tags/DOMjudge/"},{"name":"linux","slug":"linux","link":"/tags/linux/"},{"name":"sandbox","slug":"sandbox","link":"/tags/sandbox/"},{"name":"isolate","slug":"isolate","link":"/tags/isolate/"},{"name":"hexo","slug":"hexo","link":"/tags/hexo/"},{"name":"travis-ci","slug":"travis-ci","link":"/tags/travis-ci/"},{"name":"flask","slug":"flask","link":"/tags/flask/"},{"name":"crypto","slug":"crypto","link":"/tags/crypto/"}],"categories":[{"name":"Paper Notes","slug":"Paper-Notes","link":"/categories/Paper-Notes/"},{"name":"Uncategorized","slug":"Uncategorized","link":"/categories/Uncategorized/"},{"name":"Projects","slug":"Projects","link":"/categories/Projects/"},{"name":"MyOJ","slug":"Projects/MyOJ","link":"/categories/Projects/MyOJ/"}]}